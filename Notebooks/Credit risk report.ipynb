{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d181dc0c",
   "metadata": {},
   "source": [
    "# Credit Risk Prediction Report\n",
    "\n",
    "## 1. Business Problem\n",
    "Financial institutions face the challenge of predicting whether customers will default on their payments.  \n",
    "The cost of **missing a defaulter (false negative)** is often higher than the cost of **flagging a good customer as risky (false positive)**.  \n",
    "Therefore, our goal is to **maximize recall (catch as many defaulters as possible)** while still maintaining **decent precision** to avoid rejecting too many good customers.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Overview\n",
    "- Original dataset was highly imbalanced:\n",
    "  - **Non-defaulters (0): 18,691**\n",
    "  - **Defaulters (1): 5,309**\n",
    "- After applying **SMOTE (Synthetic Minority Oversampling Technique)**, classes were balanced for training:\n",
    "  - **Non-defaulters (0): 18,691**\n",
    "  - **Defaulters (1): 18,691**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Baseline Modeling (Before Feature Engineering)\n",
    "I first built models on the raw dataset (with SMOTE applied for balancing).  \n",
    "\n",
    "### Logistic Regression (baseline)\n",
    "- Recall: **62%**\n",
    "- Precision: **37%**\n",
    "- Strength: caught many defaulters.  \n",
    "- Weakness: very low precision â†’ too many false positives.\n",
    "\n",
    "### Random Forest (baseline, without SMOTE)\n",
    "- Recall: **34%**\n",
    "- Precision: **64%**\n",
    "- Strength: fewer false positives.  \n",
    "- Weakness: missed majority of defaulters.\n",
    "\n",
    "### Random Forest + SMOTE (baseline)\n",
    "- Recall: **47%**\n",
    "- Precision: **55%**\n",
    "- Best balance among the baseline models.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Engineering\n",
    "To improve predictive power, I engineered new features in addition to the original dataset.  \n",
    "Examples of engineered features include:\n",
    "- Ratios of bill amounts to credit limits.  \n",
    "- Aggregated payment delays across months.  \n",
    "- Grouping categorical levels (e.g., education, marital status).  \n",
    "\n",
    "The motivation was to better capture customer repayment behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Enhanced Modeling (After Feature Engineering)\n",
    "After creating new features, I rebuilt the models to compare performance.\n",
    "\n",
    "- **Logistic Regression (with engineered features):**\n",
    "  - Recall remained high (~62%) but precision stayed low (~37%).  \n",
    "- **Random Forest (with engineered features, no SMOTE):**\n",
    "  - Recall improved slightly but still too low to be practical.  \n",
    "- **Random Forest + SMOTE (with engineered features):**\n",
    "  - Recall ~47%, Precision ~55%.  \n",
    "  - Maintained the best trade-off even with richer features.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why Random Forest + SMOTE?\n",
    "- Logistic Regression gave the **highest recall** but at the cost of an unacceptable false positive rate.  \n",
    "- Random Forest without SMOTE improved precision but failed to catch enough defaulters.  \n",
    "- **Random Forest + SMOTE consistently provided the best compromise**, both before and after feature engineering:\n",
    "  - Better recall than plain RF.  \n",
    "  - Better precision than logistic regression.  \n",
    "  - Engineered features allowed RF to capture non-linear repayment patterns.  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Model Improvement Strategy\n",
    "- **Threshold tuning:** Adjust probability threshold to reach business-defined recall or precision.  \n",
    "- **Cost-sensitive learning:** Penalize false negatives more heavily.  \n",
    "- **Feature importance analysis:** Identify which engineered features drive model performance.  \n",
    "- **Business cost simulation:** Quantify trade-off of false positives vs false negatives in monetary terms.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Recommendation\n",
    "I recommend **deploying Random Forest with SMOTE and engineered features** as the production model.  \n",
    "It consistently balanced recall and precision across both phases of experimentation, making it the most practical choice.  \n",
    "Threshold tuning can be applied to align the model with business priorities (e.g., maximize recall in high-risk scenarios).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ef403",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
